filter {

  if ([event][provider] == "filescan") {

    if ([file][size]) {
      ruby {
        id => "ruby_filescan_file_size"
        code => "
          file_size = event.get('[file][size]').to_i
          event.set('[network][bytes]', file_size)
          event.set('[totDataBytes]', file_size)
        "
      }
    }

    # save the filescan's UUID id as into event.id
    mutate {
      id => "mutate_merge_event_id_filescan_id"
      merge => { "[event][id]" => "[id]" }
    }

    ruby {
        id => "ruby_filescan_results_processing"
        init => '
          require "yaml"
          @tactic_id_map = YAML.safe_load(File.read("/etc/mitre_attack_tactic_enterprise_ids.yaml"))
          @technique_id_map = YAML.safe_load(File.read("/etc/mitre_attack_technique_enterprise_ids.yaml"))
          @sev_enabled = ENV["LOGSTASH_SEVERITY_SCORING"] || "false";
        '
        code => "
            scanners = Array(event.get('[results][strelka][result][scanners]') || []).compact

            rules = Array(event.get('[results][strelka][result][rules]') || []).compact
            hit_scanners = rules.map { |h| h['scanner'] }.compact.uniq

            event.set('[event][module]', scanners) unless scanners.empty?
            event.set('[rule][ruleset]', hit_scanners) unless hit_scanners.empty?
            event.set('[filescan][hits]', rules.length)
            event.set('[filescan][rules]', rules.map { |h| h.slice('name', 'scanner') }) unless rules.empty?

            if rules.any?
              event.set('[event][kind]', 'alert')
              event.set('[rule][name]', rules.map { |h| h['name'] }.uniq)
              if @sev_enabled.match?(/^(true|t|yes|y|1)$/i)
                sev_tags = []
                sev_tags << 'Signature (ClamAV)' if hit_scanners.include?('clamav')
                sev_tags << 'Signature (YARA)' if hit_scanners.include?('yara')
                sev_tags << 'Signature (capa)' if hit_scanners.include?('capa')
                sev_tags << 'Signature' if sev_tags.empty?
                event.set('[event][severity_tags]', sev_tags)
              end

            else
              event.set('[event][kind]', 'event')
            end

            if hit_scanners.include?('capa')
              tactic_names     = []
              tactic_ids       = []
              tactic_refs      = []
              technique_names  = []
              technique_ids    = []
              technique_refs   = []
              Hash(event.get('[results][strelka][result][scan][capa][rules]') || {}).each do |rule_name, rule|
                meta = rule.is_a?(Hash) ? rule['meta'] : nil
                next unless meta.is_a?(Hash)

                attacks = meta['attack']
                next unless attacks.is_a?(Array)

                attacks.each do |attack|
                  next unless attack.is_a?(Hash)

                  if (tactic = attack['tactic']).is_a?(String)
                    tactic_key = tactic.gsub(/[^A-Za-z0-9]+/, '_')
                    tactic_id  = @tactic_id_map[tactic_key]
                    tactic_names |= [tactic]
                    tactic_ids   |= [tactic_id] if tactic_id
                    tactic_refs  |= [ 'https://attack.mitre.org/tactics/' + tactic_id ] if tactic_id
                  end

                  if (technique = attack['technique']).is_a?(String)
                    technique_key = technique.gsub(/[^A-Za-z0-9]+/, '_')
                    technique_id  = attack['id'] || @technique_id_map[technique_key]
                    technique_names |= [technique]
                    technique_ids   |= [technique_id] if technique_id
                    technique_refs  |= [ 'https://attack.mitre.org/techniques/' + technique_id ] if technique_id
                  end
                end
              end

              event.set('[threat][tactic][name]', tactic_names.uniq) unless tactic_names.empty?
              event.set('[threat][tactic][id]', tactic_ids.uniq) unless tactic_ids.empty?
              event.set('[threat][tactic][reference]', tactic_refs.uniq) unless tactic_refs.empty?
              event.set('[threat][technique][name]', technique_names.uniq) unless technique_names.empty?
              event.set('[threat][technique][id]', technique_ids.uniq) unless technique_ids.empty?
              event.set('[threat][technique][reference]', technique_refs.uniq) unless technique_refs.empty?
              event.set('[threat][framework]', 'MITRE ATT&CK') unless tactic_names.empty?
            end
        "
    }

    # https://www.elastic.co/guide/en/ecs/current/ecs-file.html

    # ECS - "file" -> file.type
    mutate { id => "mutate_filescan_add_field_ecs_file_type"
             add_field => { "[file][type]" => "file" } }

    if ([results][strelka][result][scan][entropy][entropy]) {
      mutate {
        id => "mutate_filescan_add_entropy"
        add_field =>  { "[file][entropy]" => "%{[results][strelka][result][scan][entropy][entropy]}" }
      }
    }

    if ([filescan][file][name]) {
      mutate {
        id => "mutate_merge_normalize_filescan_filename"
        merge => { "[file][path]" => "[filescan][file][name]" }
      }
    }

    # hashes (will be merged into "related" in 98_finalize.conf)
    # https://www.elastic.co/guide/en/ecs/current/ecs-hash.html
    # https://www.elastic.co/guide/en/ecs/current/ecs-related.html

    if ([filescan][scan][hash][md5]) {
      mutate {
        id => "mutate_merge_field_file_hash_fileinfo_md5"
        merge => { "[file][hash][md5]" => "[filescan][scan][hash][md5]" }
      }
    } else if ([zeek][files][md5]) {
      mutate {
        id => "mutate_merge_filescan_zeek_file_hash_md5"
        merge => { "[file][hash][md5]" => "[zeek][files][md5]" }
      }
    }

    if ([filescan][scan][hash][sha1]) {
      mutate {
        id => "mutate_merge_field_file_hash_fileinfo_sha1"
        merge => { "[file][hash][sha1]" => "[filescan][scan][hash][sha1]" }
      }
    } else if ([zeek][files][sha1]) {
      mutate {
        id => "mutate_merge_filescan_zeek_file_hash_sha1"
        merge => { "[file][hash][sha1]" => "[zeek][files][sha1]" }
      }
    }

    if ([filescan][scan][hash][sha256]) {
      mutate {
        id => "mutate_merge_field_file_hash_fileinfo_sha256"
        merge => { "[file][hash][sha256]" => "[filescan][scan][hash][sha256]" }
      }
    } else if ([zeek][files][sha256]) {
      mutate {
        id => "mutate_merge_filescan_zeek_file_hash_sha256"
        merge => { "[file][hash][sha256]" => "[zeek][files][sha256]" }
      }
    }

    if ([filescan][scan][hash][ssdeep]) {
      mutate {
        id => "mutate_merge_field_file_hash_fileinfo_ssdeep"
        merge => { "[file][hash][ssdeep]" => "[filescan][scan][hash][ssdeep]" }
      }
    } else if ([zeek][files][ssdeep]) {
      mutate {
        id => "mutate_merge_filescan_zeek_file_hash_ssdeep"
        merge => { "[file][hash][ssdeep]" => "[zeek][files][ssdeep]" }
      }
    }

    if ([filescan][scan][hash][tlsh]) {
      mutate {
        id => "mutate_merge_field_file_hash_fileinfo_tlsh"
        merge => { "[file][hash][tlsh]" => "[filescan][scan][hash][tlsh]" }
      }
    } else if ([zeek][files][tlsh]) {
      mutate {
        id => "mutate_merge_filescan_zeek_file_hash_tlsh"
        merge => { "[file][hash][tlsh]" => "[zeek][files][tlsh]" }
      }
    }

    if ([file][hash][md5]) {
      ruby {
        id => "ruby_filescan_file_hash_md5_make_unique"
        path => "/usr/share/logstash/malcolm-ruby/make_unique_array.rb"
        script_params => { "field" => "[file][hash][md5]" }
      }
      mutate { id => "mutate_filescan_file_hash_md5_makrelated"
               merge => { "[related][hash]" => "[file][hash][md5]" } }
    }

    if ([file][hash][sha1]) {
      ruby {
        id => "ruby_filescan_file_hash_sha1_make_unique"
        path => "/usr/share/logstash/malcolm-ruby/make_unique_array.rb"
        script_params => { "field" => "[file][hash][sha1]" }
      }
      mutate { id => "mutate_filescan_file_hash_sha1_marelated"
               merge => { "[related][hash]" => "[file][hash][sha1]" } }
    }

    if ([file][hash][sha256]) {
      ruby {
        id => "ruby_filescan_file_hash_sha256_make_unique"
        path => "/usr/share/logstash/malcolm-ruby/make_unique_array.rb"
        script_params => { "field" => "[file][hash][sha256]" }
      }
      mutate { id => "mutate_filescan_file_hash_sha256_related"
               merge => { "[related][hash]" => "[file][hash][sha256]" } }
    }

    if ([file][hash][ssdeep]) {
      ruby {
        id => "ruby_filescan_file_hash_ssdeep_make_unique"
        path => "/usr/share/logstash/malcolm-ruby/make_unique_array.rb"
        script_params => { "field" => "[file][hash][ssdeep]" }
      }
      mutate { id => "mutate_filescan_file_hash_ssdeep_related"
               merge => { "[related][hash]" => "[file][hash][ssdeep]" } }
    }

    if ([file][hash][tlsh]) {
      ruby {
        id => "ruby_filescan_file_hash_tlsh_make_unique"
        path => "/usr/share/logstash/malcolm-ruby/make_unique_array.rb"
        script_params => { "field" => "[file][hash][tlsh]" }
      }
      mutate { id => "mutate_filescan_file_hash_tlsh_related"
               merge => { "[related][hash]" => "[file][hash][tlsh]" } }
    }

    if ([file][mime_type]) {
      ruby {
        id => "ruby_filescan_mime_type_adjust_and_unique"
        code => "
          mimeTypes = Array(event.get('[file][mime_type]') || []).compact.map do |mt|
            mt = mt.to_s.strip
            mt.split(';')[0].strip.downcase
          end
          event.set('[file][mime_type]', mimeTypes.uniq.reject(&:empty?))
        "
      }
    }

    # File/MIME types ###################################################################################################
    # ECS -> various -> file.mime_type
    # collect all file/MIME types under the parent [file][mime_type] array

    if ([filescan][file][flavors][mime]) {
      mutate {
        id => "mutate_merge_normalize_filescan_fileinfo_mime_type"
        merge => { "[file][mime_type]" => "[filescan][file][flavors][mime]" }
      }
    }
    if ([results][strelka][result][file][mime_type]) {
      mutate {
        id => "mutate_merge_normalize_filescan_result_file_mime_type"
        merge => { "[file][mime_type]" => "[results][strelka][result][file][mime_type]" }
      }
    }
    if ([results][strelka][result][scan][exiftool][mime_type]) {
      mutate {
        id => "mutate_merge_normalize_filescan_result_scan_exiftool_mime_type"
        merge => { "[file][mime_type]" => "[results][strelka][result][scan][exiftool][mime_type]" }
      }
    }

    # from here a lot of this stuff mimics what is done for zeek's files.log, only from the record that came from the filescan metadata

    if [@metadata][log_file_type] {
      # move "up" some fields that are considered to be "multi-log" fields (eg., they show up in many types of logs)
      mutate {
        id => "mutate_rename_filescan_zeek_common_fields"
        rename => { "[zeek][%{[@metadata][log_file_type]}][ts]"         => "[zeek][ts]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][uid]"        => "[zeek][uid]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][fuid]"       => "[zeek][fuid]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][is_orig]"    => "[network][is_orig]" }
        # this seems redundant but depending on how it came here it might look like any one of these
        rename => { "[zeek][%{[@metadata][log_file_type]}][id][orig_h]" => "[source][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id][orig_p]" => "[source][port]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id][resp_h]" => "[destination][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id][resp_p]" => "[destination][port]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id.orig_h]"  => "[source][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id.orig_p]"  => "[source][port]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id.resp_h]"  => "[destination][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][id.resp_p]"  => "[destination][port]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][orig_h]"     => "[source][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][orig_p]"     => "[source][port]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][resp_h]"     => "[destination][ip]" }
        rename => { "[zeek][%{[@metadata][log_file_type]}][resp_p]"     => "[destination][port]" }
      }
    }

    # store parent_fuid with fuid
    if ([zeek][files][parent_fuid]) {
      mutate { id => "mutate_filescan_merge_normalize_zeek_files_parent_fuid"
               merge => { "[zeek][fuid]" => "[zeek][files][parent_fuid]" } }
    }

    if ([zeek][uid]) {
      # set zeek connection UID as "rootId" (see logstash.conf output section)
      if (![rootId]) { mutate { id => "mutate_filescan_copy_zeek_rootId"
                                copy => { "[zeek][uid]" => "[rootId]" } } }

      # ECS - zeek.uid -> event.id
      mutate { id => "mutate_filescan_merge_ecs_id_uid"
               merge => { "[event][id]" => "[zeek][uid]" } }
    }

    if ([zeek][fuid]) {
      # ECS - zeek.uid -> event.id
      mutate { id => "mutate_filescan_merge_ecs_id_fuid"
               merge => { "[event][id]" => "[zeek][fuid]" } }
    }

    if ([zeek][files][source]) {
      # do some normalization on files source
      mutate { id => "mutate_gsub_field_filescan_zeek_files_source_spicy_suffix"
               gsub => [ "[zeek][files][source]", "_(TCP|UDP|DATA)$", "" ] }
      mutate { id => "mutate_gsub_field_filescan_zeek_files_source_spicy_prefix"
               gsub => [ "[zeek][files][source]", "^SPICY_", "" ] }

      if ([zeek][files][source] =~ /^XOR decrypted from /) {
        # PE_XOR plugin writes source as "XOR decrypted from FM7Tr545kxt3ofR7x2-"..., let's
        # standardize that to just "XOR decrypted" and put the source fuid in parent_fuid
        grok {
          id => "grok_filescan_zeek_files_source_xor"
          match => { "[zeek][files][source]" => [ "decrypted%{SPACE}from%{SPACE}%{WORD:[@metadata][source_xor_fuid]}" ] }
        }
        if ([@metadata][source_xor_fuid]) { mutate { id => "mutate_merge_filescan_zeek_files_source_xor_parent_fuid"
                                                     merge => { "[zeek][files][parent_fuid]" => "[@metadata][source_xor_fuid]" } } }
        mutate { id => "mutate_replace_filescan_zeek_files_source_xor"
                 replace => { "[zeek][files][source]" => "XOR decrypted" } }

      } else if ([zeek][files][source] == "SSL") {
        # SSL->TLS to match up with what the service field has for cross-referencing
        mutate { id => "mutate_replace_filescan_zeek_files_source_ssl_to_tls"
                 replace => { "[zeek][files][source]" => "TLS" } }
      }

      # ECS -> various -> file.source
      mutate { id => "mutate_filescan_rename_file_source"
               rename => { "[zeek][files][source]" => "[file][source]" } }

      if ([zeek][files][mime_type]) {
        mutate {
          id => "mutate_merge_normalize_filescan_zeek_files_mime_type"
          merge => { "[file][mime_type]" => "[zeek][files][mime_type]" }
        }
      }

      # if we have a network protocol as a "source" field from the original zeek files.log, also store in protocols
      if (![network][protocol]) and
         ([zeek][files][source]) and
         ([zeek][files][source] !~ /^(<error|PNG|XOR|ZIP)/) {
        mutate { id => "mutate_filescan_merge_zeek_files_source_network_protocol"
                 merge => { "[network][protocol]" => "[zeek][files][source]" } }
        mutate { id => "mutate_filescan_lowercase_zeek_files_source_network_protocol"
                 lowercase => [ "[network][protocol]" ] }
        mutate { id => "mutate_filescan_merge_zeek_files_source_protocol"
                 merge => { "[protocol]" => "[network][protocol]" } }
      }
    }

    # extracted_uri link
    if ([file][path]) {
      mutate { id => "mutate_gsub_filescan_path"
               gsub => [ "[file][path]", "^/zeek/extract_files/", "" ] }
      ruby {
        id => "ruby_filescan_zeek_files_extracted_uri_build"
        # don't store extracted_uri if the file probably wasn't preserved
        init => "@preservation = ENV['FILESCAN_PRESERVATION'] || 'all'"
        code => "
          if ((@preservation == 'all') ||
              ((@preservation == 'quarantined') && (event.get('[filescan][hits]').to_i > 0))) &&
             (fName = event.get('[file][path]')) then
            uri = nil
            if (tags = event.get('[tags]')) && tags.include?('_filebeat_zeek_hedgehog') then
              if (hName = event.get('[host][name]')) then
                uri = 'hh-extracted-files/' + hName + '/' + fName
              end
            else
              uri = 'extracted-files/' + fName
            end
            event.set('[zeek][files][extracted_uri]', uri) unless uri.to_s.empty?
          end
        "
        remove_field => [ "[zeek][files][extracted]" ]
      }
    }
  }

  # timestamp calculation
  ruby {
    id => "ruby_filescan_timestamp_calc"
    init => "
      require 'date'
      @parse_to_ms = ->(v) {
        begin
          v && !v.to_s.strip.empty? ?
            (DateTime.parse(v).to_time.to_f * 1000).round :
            nil
        rescue
          nil
        end
      }
    "
    code => "
      ts_val    = event.get('[@timestamp]').to_s
      start_ms = @parse_to_ms.call(event.get('[zeek][ts]')) || @parse_to_ms.call(event.get('[entity][raw][timestamp]')) || @parse_to_ms.call(event.get('[start]')) || @parse_to_ms.call(ts_val)
      end_ms   = @parse_to_ms.call(event.get('[end]')) || start_ms
      event.set('[firstPacket]', start_ms)
      event.set('[lastPacket]',  end_ms)
    "
    remove_field => [ "[start]", "[end]" ]
  }

}
