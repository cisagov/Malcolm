# Copyright (c) 2025 Battelle Energy Alliance, LLC.  All rights reserved.

# final adjustments before forwarding

filter {

    # generate opensearch index name
    if (![@metadata][malcolm_opensearch_index]) {
      ruby {
        id => "ruby_resolve_network_logs_index"
        path => "/usr/share/logstash/malcolm-ruby/format_index_string.rb"
        script_params => {
          "target" => "[@metadata][malcolm_opensearch_index]"
          "prefix_env" => "MALCOLM_NETWORK_INDEX_PATTERN"
          "prefix_default" => "arkime_sessions3-*"
          "suffix_env" => "MALCOLM_NETWORK_INDEX_SUFFIX"
          "suffix_default" => "%{%y%m%d}"
        }
      }
    }

    # convert @timestamp (a Logstash::Timestamp object) to epoch-milliseconds
    # and add to the correct fields so that results have a populated timestamp
    # in Arkime, and are visible at all in Opensearch Dashboards
    ruby {
      id => "ruby_suricata_timestamp_calc"
      init => "require 'time'; require 'date'"
      code => "
        timestamp = DateTime.parse(event.get('[@timestamp]').to_iso8601).to_time
        timestamp_ms = (1000 * timestamp.to_f).round(0)
        event.set('[firstPacket]', timestamp_ms)
        event.set('[lastPacket]', timestamp_ms)
      "
    }

    # if we don't already have a provider/dataset, tack on a default
    if (![event][provider]) {
      mutate {
        id => "event_provider_fallback"
        add_field => { "[event][provider]" => "filescan" }
      }
    }
    if (![event][dataset]) {
      if ([results][strelka]) {
        mutate {
          id => "filescan_event_dataset_strelka"
          add_field => { "[event][dataset]" => "strelka" }
        }
      } else {
        mutate {
          id => "filescan_event_dataset_unknown"
          add_field => { "[event][dataset]" => "unknown" }
        }
      }
    }

    # arkime doesn't like / in the record ID
    mutate { id => "mutate_gsub_event_hash_urlsafe"
             gsub => [ "[event][hash]", "/", "_",
                       "[event][hash]", "\+", "-",
                       "[event][hash]", "=+", "" ] }

    # trim path portion of originating log file
    if ([log][file][path]) { mutate { id => "mutate_gsub_log_file_path_directory"
                                      gsub => [ "[log][file][path]", "^.*/", "" ] } }


    mutate { id => "mutate_filescan_final_tags_remove"
             remove_tag => [ "_dateparsefailure",
                             "_filebeat_filescan",
                             "_filebeat_filescan_hedgehog",
                             "_filebeat_filescan_live",
                             "_filebeat_filescan_malcolm_live",
                             "_filebeat_filescan_malcolm_upload",
                             "_filebeat_filescan_upload",
                             "_grokparsefailure",
                             "_jsonparsefailure",
                             "_jsonparsesuccess" ] }


    # remove some fields we don't need (or don't need anymore)
    mutate {
      id => "mutate_filescan_remove_field_useless"
      remove_field => [
        "[beat]",
        "[agent][ephemeral_id]",
        "[agent][id]",
        "[agent][type]",
        "[agent][version]",
        "[input][type]",
        "[log][offset]",
        "[prospector]",
        "[metadata][record]",
        "[results][strelka][result][file][metadata]"
      ]
    }

}
