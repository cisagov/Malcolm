filter {
  # AWS S3 Access Logs Parser
  # Parses S3 bucket access logs
  # Related: GitHub Issue #232 - Cloud Infrastructure Logs Integration
  
  if ([event][dataset] == "aws.s3access") {
    
    # S3 access logs are space-delimited with some quoted fields
    # Format: https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html
    
    grok {
      id => "grok_s3_access_logs"
      match => {
        "message" => "%{NOTSPACE:aws.s3.bucket_owner} %{NOTSPACE:aws.s3.bucket} \[%{HTTPDATE:aws.s3.timestamp}\] %{NOTSPACE:source.ip} %{NOTSPACE:aws.s3.requester} %{NOTSPACE:aws.s3.request_id} %{NOTSPACE:aws.s3.operation} %{NOTSPACE:aws.s3.key} \"(?:%{WORD:http.request.method} %{NOTSPACE:url.original}(?: HTTP/%{NUMBER:http.version})?|%{DATA:aws.s3.request_uri})\" %{NUMBER:http.response.status_code:int} %{NOTSPACE:aws.s3.error_code} %{NUMBER:http.response.bytes:int} %{NUMBER:aws.s3.object_size:int} %{NUMBER:aws.s3.total_time:int} %{NUMBER:aws.s3.turn_around_time:int} \"%{DATA:http.request.referrer}\" \"%{DATA:user_agent.original}\" %{NOTSPACE:aws.s3.version_id} %{NOTSPACE:aws.s3.host_id} %{NOTSPACE:tls.version_protocol} %{NOTSPACE:tls.cipher} %{NOTSPACE:aws.s3.authentication_type} %{NOTSPACE:aws.s3.host_header} %{NOTSPACE:tls.client_server_name} %{NOTSPACE:aws.s3.signature_version} %{NOTSPACE:tls.cipher_suite} %{NOTSPACE:aws.s3.access_point_arn} %{NOTSPACE:aws.s3.acl_required}"
      }
      tag_on_failure => ["_grokparsefailure_s3"]
    }
    
    # Parse timestamp
    if [aws.s3.timestamp] {
      date {
        id => "date_s3_timestamp"
        match => [ "[aws.s3][timestamp]", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "@timestamp"
      }
    }
    
    # Parse URL components
    if [url.original] {
      ruby {
        id => "ruby_s3_parse_url"
        code => "
          require 'uri'
          begin
            uri = URI.parse(event.get('[url][original]'))
            event.set('[url][path]', uri.path) if uri.path
            event.set('[url][query]', uri.query) if uri.query
            event.set('[url][scheme]', uri.scheme) if uri.scheme
          rescue
            # Invalid URL, skip parsing
          end
        "
      }
    }
    
    # Extract file extension from key
    if [aws.s3.key] and [aws.s3.key] != "-" {
      grok {
        id => "grok_s3_file_extension"
        match => { "[aws.s3][key]" => "\.(?<file.extension>[^.]+)$" }
      }
      
      mutate {
        id => "mutate_s3_file_name"
        add_field => { "[file][path]" => "%{[aws.s3][key]}" }
      }
    }
    
    # Set event outcome based on status code
    if [http.response.status_code] {
      if [http.response.status_code] >= 200 and [http.response.status_code] < 400 {
        mutate {
          id => "mutate_s3_success"
          add_field => { "[event][outcome]" => "success" }
        }
      } else {
        mutate {
          id => "mutate_s3_failure"
          add_field => { 
            "[event][outcome]" => "failure"
            "[event][type]" => "error"
          }
        }
      }
    }
    
    # Classify operations
    if [aws.s3.operation] {
      if [aws.s3.operation] =~ /^(GET|HEAD)\./ {
        mutate {
          id => "mutate_s3_read_operation"
          add_field => { "[event][action]" => "read" }
        }
      } else if [aws.s3.operation] =~ /^(PUT|POST|COPY)\./ {
        mutate {
          id => "mutate_s3_write_operation"
          add_field => { "[event][action]" => "write" }
          add_tag => ["data_upload"]
        }
      } else if [aws.s3.operation] =~ /^DELETE\./ {
        mutate {
          id => "mutate_s3_delete_operation"
          add_field => { "[event][action]" => "delete" }
          add_tag => ["data_deletion"]
        }
      } else if [aws.s3.operation] =~ /^(LIST|BUCKET)\./ {
        mutate {
          id => "mutate_s3_list_operation"
          add_field => { "[event][action]" => "list" }
        }
      }
    }
    
    # Add cloud metadata
    mutate {
      id => "mutate_s3_cloud_metadata"
      add_field => {
        "[cloud][provider]" => "aws"
        "[cloud][service][name]" => "s3"
        "[event][kind]" => "event"
        "[event][category]" => "file"
      }
    }
    
    # Detect large downloads (> 100MB)
    if [http.response.bytes] and [http.response.bytes] > 104857600 {
      mutate {
        id => "mutate_s3_large_download"
        add_tag => ["large_download"]
      }
    }
    
    # Detect large uploads (> 100MB)
    if [aws.s3.object_size] and [aws.s3.object_size] > 104857600 {
      mutate {
        id => "mutate_s3_large_upload"
        add_tag => ["large_upload"]
      }
    }
    
    # Detect access denied
    if [http.response.status_code] == 403 {
      mutate {
        id => "mutate_s3_access_denied"
        add_tag => ["access_denied"]
      }
    }
    
    # Detect anonymous access
    if [aws.s3.requester] == "-" or [aws.s3.authentication_type] == "Anonymous" {
      mutate {
        id => "mutate_s3_anonymous"
        add_tag => ["anonymous_access"]
      }
    }
    
    # Detect public bucket access (potential data leak)
    if [aws.s3.acl_required] == "Yes" {
      mutate {
        id => "mutate_s3_public_access"
        add_tag => ["public_bucket_access"]
      }
    }
    
    # Detect sensitive file extensions
    if [file.extension] =~ /^(pem|key|p12|pfx|crt|cer|der|env|config|conf|sql|db|sqlite|bak|backup)$/i {
      mutate {
        id => "mutate_s3_sensitive_file"
        add_tag => ["sensitive_file_access"]
      }
    }
    
    # Clean up
    mutate {
      id => "mutate_s3_cleanup"
      remove_field => ["message"]
      # Replace "-" with null for optional fields
      gsub => [
        "[aws.s3][requester]", "^-$", "",
        "[aws.s3][error_code]", "^-$", "",
        "[aws.s3][version_id]", "^-$", "",
        "[tls][version_protocol]", "^-$", "",
        "[tls][cipher]", "^-$", ""
      ]
    }
    
    # Note: GeoIP enrichment for source.ip will be handled by Malcolm's
    # existing 11_lookups.conf filter
    
  } # end if aws.s3access

} # end filter
